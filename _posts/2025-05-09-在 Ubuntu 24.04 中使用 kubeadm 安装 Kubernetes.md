---
title: 在 Ubuntu 24.04 中使用 kubeadm 安装 Kubernetes
description: >-
  记录在 Ubuntu 24.04 中使用 kubeadm 安装 Kubernetes
author: panhuida
date: 2025-05-09 16:45 +0800
categories: [Blog, Tutorial]
tags: [homelab]
pin: true
media_subpath: /assets/img
---



为了在 Kubernetes 使用 Airflow，在本地 （迷你主机）搭建 Kubernetes 集群 。

Kubernetes 是容器编排，Airflow 是任务编排。



### 01 介绍

Kubernetes 是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，方便进行声明式配置和自动化。

Kubernetes 这个名字源于希腊语，意为“舵手”或“飞行员”。K8s 这个缩写是因为 K 和 s 之间有 8 个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目。 

Kubernetes 建立在 Google 大规模运行生产工作负载十几年经验的基础上， 结合了**社区中最优秀的想法和实践**。



本文记录的是在本地（迷你主机）搭建 Kubernetes 集群。

[迷你主机安装的操作系统是 Ubuntu 24.04](https://mp.weixin.qq.com/s/ljKiQZY9HsobYygfTHvKCA)，在Ubuntu 24.04 上使用 Multipass 创建 3个虚拟机，在虚拟机上使用 Kubeadm 安装 Kubernetes 集群。

安装步骤按照官方文档进行，但调整了一些顺序，另外一些说明如下：

- 有些章节只有说明或提示，没有操作，这个按照官方文档的顺序保留下来的。
- 有些命令的输出结果保留（见输出示例），方便自己理解。
- 有些知识点在“（）”中补充，方便自己理解。



官方文档

https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/



### 02 部署视图

Kubernetes 版本为  v1.32.3

![image-20250508204858489](2025-05-09-在 Ubuntu 24.04 中使用 kubeadm 安装 Kubernetes.assets/image-20250508204858489.png)



| 虚拟机名称 | 用途              | 配置  | 备注                      |
| :--------- | :---------------- | ----- | ------------------------- |
| Node 1     | K8s master 节点   | 2C 8G | 主机总共16 C，建议分配 6C |
| Node 2     | K8s worker 节点 1 | 2C 8G | 主机总共16 C，建议分配 3C |
| Node 3     | K8s worker 节点 2 | 2C 8G | 主机总共16 C，建议分配 3C |





### 03 操作步骤

#### 1. 使用 Multipass 创建虚拟机

Multipass 是一个灵活而强大的工具，可用于多种用途。在最简单的形式中，可以使用它在任何主机上快速创建和销毁 Ubuntu VM（实例）。也可以使用 Multipass 在笔记本电脑上构建本地迷你云，以测试和开发多实例或基于容器的云应用程序。

https://documentation.ubuntu.com/multipass/en/latest/how-to-guides/install-multipass/

```shell
pan@pan-SER8:~$ snap info multipass
pan@pan-SER8:~$ sudo snap install multipass
pan@pan-SER8:~$ multipass version
pan@pan-SER8:~$ multipass help

# 查看有哪些可用的镜像
pan@pan-SER8:~$ multipass find

# 节点
# 从使用来看，CPU 配置为 6, 3, 3 可能会更好
pan@pan-SER8:~$ multipass launch 24.04 --name node1 --cpus 2 --memory 8G --disk 20G
pan@pan-SER8:~$ multipass launch 24.04 --name node2 --cpus 2 --memory 8G --disk 20G
pan@pan-SER8:~$ multipass launch 24.04 --name node3 --cpus 2 --memory 8G --disk 20G

pan@pan-SER8:~$ multipass list 
pan@pan-SER8:~$ multipass info node1
pan@pan-SER8:~$ multipass shell node1
pan@pan-SER8:~$ multipass stop node1
# 删除
pan@pan-SER8:~$ multipass delete node1
# 恢复
pan@pan-SER8:~$ multipass recover primary
# 永久删除
pan@pan-SER8:~$ multipass purge
```

输出示例

```shell
pan@pan-SER8:~$ multipass list
Name                    State             IPv4             Image
node1                   Running           10.227.94.45     Ubuntu 24.04 LTS
node2                   Running           10.227.94.229    Ubuntu 24.04 LTS
node3                   Running           10.227.94.236    Ubuntu 24.04 LTS
```

multipass 界面

![image-20250412225839382](2025-05-09-在 Ubuntu 24.04 中使用 kubeadm 安装 Kubernetes.assets/image-20250412225839382.png)





#### 2. 安装和配置先决条件

> **说明**
>
> 安装和配置先决条件，整合了如下页面中的内容
>
> 容器运行时
>
> https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/
>
> 安装 kubeadm
>
> https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/



##### （1）准备开始

- 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令。
- 每台机器 2 GB 或更多的 RAM（如果少于这个数字将会影响你应用的运行内存）。
- 控制平面机器需要 CPU 2 核心或更多。
- 集群中的所有机器的网络彼此均能相互连接（公网和内网都可以）。
- 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见[这里](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address)了解更多详细信息。
- 开启机器上的某些端口。请参见[这里](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports)了解更多详细信息。
- 交换分区的配置。kubelet 的默认行为是在节点上检测到交换内存时无法启动。



##### （2）确保每个节点上 MAC 地址和 product_uuid 的唯一性

- 你可以使用命令 `ip link` 或 `ifconfig -a` 来获取网络接口的 MAC 地址
- 可以使用 `sudo cat /sys/class/dmi/id/product_uuid` 命令对 product_uuid 校验

一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装[失败](https://github.com/kubernetes/kubeadm/issues/31)。



##### （3）检查网络适配器

如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则， 这样 Kubernetes 集群就可以通过对应的适配器完成连接。



##### （4）检查所需端口

启用这些[必要的端口](https://kubernetes.io/zh-cn/docs/reference/networking/ports-and-protocols/)后才能使 Kubernetes 的各组件相互通信。 可以使用 [netcat](https://netcat.sourceforge.net/) 之类的工具来检查端口是否开放。



##### （5）交换分区的配置

kubelet 的默认行为是在节点上检测到交换内存时无法启动。  这意味着要么禁用交换（swap）功能，要么让 kubelet 容忍交换。

本机已经禁用交换。



##### （6）启用 IPv4 数据包转发

默认情况下，Linux 内核不允许 IPv4 数据包在接口之间路由。

```shell
# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 验证 net.ipv4.ip_forward 是否设置为 1
sysctl net.ipv4.ip_forward
```

加载 br_netfilter，避免 kube-flannel pod 出现异常

```shell
# 手动加载内核模块
sudo modprobe br_netfilter

# 确认模块已经加载
lsmod | grep br_netfilter

# 写入配置文件，系统重启后依然生效
cat <<EOF | sudo tee -a /etc/modules-load.d/k8s.conf
br_netfilter
EOF

# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee -a /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 确认参数的设置
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```



#### 3. 安装容器运行时 containerd

https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/

选择 containerd



##### （1）cgroup 驱动

在 Linux 上，[控制组（CGroup）](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-cgroup)用于限制分配给进程的资源。

[kubelet](https://kubernetes.io/zh-cn/docs/reference/generated/kubelet) 和底层容器运行时都需要对接控制组来强制执行 [为 Pod 和容器管理资源](https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/) 并为诸如 CPU、内存这类资源设置请求和限制。若要对接控制组，kubelet 和容器运行时需要使用一个 **cgroup 驱动**。 关键的一点是 kubelet 和容器运行时需使用相同的 cgroup 驱动并且采用相同的配置。

可用的 cgroup 驱动有两个：

- [`cgroupfs`](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroupfs-cgroup-driver)
- [`systemd`](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#systemd-cgroup-driver)



当 [systemd](https://www.freedesktop.org/wiki/Software/systemd/) 是初始化系统时， **不** 推荐使用 `cgroupfs` 驱动，因为 systemd 期望系统上只有一个 cgroup 管理器。 此外，如果你使用 [cgroup v2](https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups)， 则应用 `systemd` cgroup 驱动取代 `cgroupfs`。

> 说明：
>
> 从 v1.22 开始，在使用 kubeadm 创建集群时，如果用户没有在 `KubeletConfiguration` 下设置 `cgroupDriver` 字段，kubeadm 默认使用 `systemd`。



(

关于 cgroup v2

https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups/

[kubelet](https://kubernetes.io/zh-cn/docs/reference/generated/kubelet) 和底层容器运行时都需要对接 cgroup 来强制执行[为 Pod 和容器管理资源](https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/)， 这包括为容器化工作负载配置 CPU/内存请求和限制。

cgroup v2 具有以下要求：

- kubelet 和容器运行时被配置为使用 systemd cgroup 驱动

)



##### （2）安装容器运行时 containerd

**Step 1: Installing containerd**

https://github.com/containerd/containerd/blob/main/docs/getting-started.md

https://github.com/containerd/containerd/blob/main/docs/containerd-2.0.md

https://github.com/containerd/containerd/releases/tag/v2.0.4

```shell
# 配置代理
export HTTP_PROXY=http://192.168.31.72:7890/
export HTTPS_PROXY=http://192.168.31.72:7890/
# 下载
ubuntu@node1:~$ curl -L -O https://github.com/containerd/containerd/releases/download/v2.0.4/containerd-2.0.4-linux-amd64.tar.gz
ubuntu@node1:~$ curl -L -O https://github.com/containerd/containerd/releases/download/v2.0.4/containerd-2.0.4-linux-amd64.tar.gz.sha256sum
# 验证
ubuntu@node1:~$ cat containerd-2.0.4-linux-amd64.tar.gz.sha256sum 
e1c64c5fd60ecd555e750744eaef150b6f78d7f750da5c08c52825aa6b791737  containerd-2.0.4-linux-amd64.tar.gz
ubuntu@node1:~$ sha256sum containerd-2.0.4-linux-amd64.tar.gz
e1c64c5fd60ecd555e750744eaef150b6f78d7f750da5c08c52825aa6b791737  containerd-2.0.4-linux-amd64.tar.gz
# 解压
ubuntu@node1:~$ sudo tar Cxzvf /usr/local containerd-2.0.4-linux-amd64.tar.gz
```

输出示例

```shell
ubuntu@node1:~$ sudo tar Cxzvf /usr/local containerd-1.6.23-linux-amd64.tar.gz
bin/
bin/ctr
bin/containerd-stress
bin/containerd
bin/containerd-shim
bin/containerd-shim-runc-v1
bin/containerd-shim-runc-v2

# 20250413
ubuntu@node1:~$ sudo tar Cxzvf /usr/local containerd-2.0.4-linux-amd64.tar.gz
bin/
bin/ctr
bin/containerd-stress
bin/containerd
bin/containerd-shim-runc-v2
```



- systemd 配置

If you intend to **start containerd via systemd**, you should also download the `containerd.service` unit file from https://raw.githubusercontent.com/containerd/containerd/main/containerd.service into `/usr/local/lib/systemd/system/containerd.service`, and run the following commands:

If you intend to start containerd via systemd, you should also download the `containerd.service` unit file from https://raw.githubusercontent.com/containerd/containerd/main/containerd.service into `/usr/local/lib/systemd/system/containerd.service`, and run the following commands:

```shell
ubuntu@node1:~$ curl -L -O https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
ubuntu@node1:~$ sudo mkdir -p /usr/local/lib/systemd/system/
ubuntu@node1:~$ sudo mv containerd.service /usr/local/lib/systemd/system/containerd.service

ubuntu@node1:~$ sudo systemctl daemon-reload
# --now: 在启用服务的同时并立即启动
ubuntu@node1:~$ sudo systemctl enable --now containerd
```

输出示例

```shell
ubuntu@node1:~$ sudo systemctl enable --now containerd
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/local/lib/systemd/system/containerd.service.
```



**Step 2: Installing runc**

Download the `runc.<ARCH>` binary from https://github.com/opencontainers/runc/releases , verify its sha256sum, and install it as `/usr/local/sbin/runc`.

```shell
# 下载
ubuntu@node1:~$ curl -L -O https://github.com/opencontainers/runc/releases/download/v1.2.6/runc.amd64
ubuntu@node1:~$ curl -L -O https://github.com/opencontainers/runc/releases/download/v1.2.6/runc.sha256sum
ubuntu@node1:~$ cat runc.sha256sum 
ubuntu@node1:~$ sha256sum runc.amd64

# 解压
ubuntu@node1:~$ sudo install -m 755 runc.amd64 /usr/local/sbin/runc
```



**Step 3: Installing CNI plugins**

Download the `cni-plugins-<OS>-<ARCH>-<VERSION>.tgz` archive from https://github.com/containernetworking/plugins/releases , verify its sha256sum, and extract it under `/opt/cni/bin`:

```shell
ubuntu@node1:~$ curl -L -O https://github.com/containernetworking/plugins/releases/download/v1.6.2/cni-plugins-linux-amd64-v1.6.2.tgz
ubuntu@node1:~$ curl -L -O https://github.com/containernetworking/plugins/releases/download/v1.6.2/cni-plugins-linux-amd64-v1.6.2.tgz.sha256
ubuntu@node1:~$ cat cni-plugins-linux-amd64-v1.6.2.tgz.sha256 
ubuntu@node1:~$ sha256sum cni-plugins-linux-amd64-v1.6.2.tgz

# 解压
ubuntu@node1:~$ sudo mkdir -p /opt/cni/bin
ubuntu@node1:~$ sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.6.2.tgz
```

输出示例

```shell
ubuntu@node1:~$ sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.6.2.tgz
./
./ipvlan
./tap
./loopback
./host-device
./README.md
./portmap
./ptp
./vlan
./bridge
./firewall
./LICENSE
./macvlan
./dummy
./bandwidth
./vrf
./tuning
./static
./dhcp
./host-local
./sbr
```



**Step 4: Customizing containerd**

containerd uses a configuration file located in `/etc/containerd/config.toml` for specifying daemon level options. A sample configuration file can be found [here](https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.toml.5.md).

The default configuration can be generated via `containerd config default > /etc/containerd/config.toml`.

```shell
ubuntu@node1:~$ sudo mkdir -p /etc/containerd
ubuntu@node1:~$ sudo touch /etc/containerd/config.toml
ubuntu@node1:~$ sudo chmod +777 /etc/containerd/config.toml
ubuntu@node1:~$ sudo containerd config default > /etc/containerd/config.toml
```



在 Linux 上，containerd 的默认 CRI 套接字是 /run/containerd/containerd.sock。 

```shell
ubuntu@node1:~$ sudo systemctl status containerd
● containerd.service - containerd container runtime
     Loaded: loaded (/usr/local/lib/systemd/system/containerd.service; enabled; preset: enabled)
     Active: active (running) since Sun 2025-04-13 10:40:32 CST; 41min ago
       Docs: https://containerd.io
    Process: 2599 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
   Main PID: 2600 (containerd)
      Tasks: 7
     Memory: 13.5M (peak: 14.6M)
        CPU: 5.887s
     CGroup: /system.slice/containerd.service
             └─2600 /usr/local/bin/containerd
```



##### （3）配置 systemd cgroup 驱动

结合 `runc` 使用 `systemd` cgroup 驱动，在 `/etc/containerd/config.toml` 中设置：

```shell
# 增加 SystemdCgroup = true
ubuntu@node1:~$ sudo vim /etc/containerd/config.toml
          [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc.options]
            SystemdCgroup = true
```



> **说明：**
>
> 如果你从软件包（例如，RPM 或者 `.deb`）中安装 containerd，你可能会发现其中默认禁止了 CRI 集成插件。
>
> 你需要启用 CRI 支持才能在 Kubernetes 集群中使用 containerd。 要确保 `cri` 没有出现在 `/etc/containerd/config.toml` 文件中 `disabled_plugins` 列表内。如果你更改了这个文件，也请记得要重启 `containerd`。
>
> 如果你在初次安装集群后或安装 CNI 后遇到容器崩溃循环，则随软件包提供的 containerd 配置可能包含不兼容的配置参数。考虑按照 [getting-started.md](https://github.com/containerd/containerd/blob/main/docs/getting-started.md#advanced-topics) 中指定的 `containerd config default > /etc/containerd/config.toml` 重置 containerd 配置，然后相应地设置上述配置参数。



如果你应用此更改，请确保重新启动 containerd：

```shell
ubuntu@node1:~$ sudo systemctl restart containerd
```



##### （4）重载沙箱（pause）镜像

```shell
# 增加 sandbox_image = "registry.k8s.io/pause:3.10"
ubuntu@node1:~$ sudo vim /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = 'registry.k8s.io/pause:3.10'

# 重启 containerd
ubuntu@node1:~$ sudo systemctl restart containerd  
```



> **备注**
>
> 不配置重载沙箱（pause）镜像的话，使用 kubeadm 创建集群时会有警告
>
> ubuntu@node1:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
> W0413 16:13:37.336214    4537 checks.go:846] detected that the sandbox image "" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.10" as the CRI sandbox image.



#### 4. 安装 kubeadm、kubelet 和 kubectl

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

你需要在**每台机器**上安装以下的软件包：

- `kubeadm`：用来初始化集群的指令。
- `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等。
- `kubectl`：用来与集群通信的命令行工具。

kubeadm **不能**帮你安装或者管理 `kubelet` 或 `kubectl`， 所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。 然而，控制平面与 kubelet 之间可以存在**一个**次要版本的偏差，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。



##### （1）基于 Debian 的发行版

以下指令适用于 Kubernetes 1.32.

1.更新 `apt` 包索引并安装使用 Kubernetes `apt` 仓库所需要的包：

```shell
sudo apt-get update
# apt-transport-https 可能是一个虚拟包（dummy package）；如果是的话，你可以跳过安装这个包
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```

2.下载用于 Kubernetes 软件包仓库的公共签名密钥。所有仓库都使用相同的签名密钥，因此你可以忽略URL中的版本：

```shell
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# 如果 `/etc/apt/keyrings` 目录不存在，则应在 curl 命令之前创建它，请阅读下面的注释。
ls /etc/apt/keyrings
sudo mkdir -p -m 755 /etc/apt/keyrings
# 网络不通的话配置代理
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```

3.添加 Kubernetes `apt` 仓库。请注意，此仓库仅包含适用于 Kubernetes 1.32 的软件包。

```shell
# 此操作会覆盖 /etc/apt/sources.list.d/kubernetes.list 中现存的所有配置。
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

4.更新 `apt` 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：

```shell
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

输出示例

```shell
ubuntu@node1:~$ sudo apt-get install kubelet kubeadm kubectl
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  conntrack cri-tools kubernetes-cni
The following NEW packages will be installed:
  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni
  
ubuntu@node1:~$ sudo apt-mark hold kubelet kubeadm kubectl
kubelet set on hold.
kubeadm set on hold.
kubectl set on hold.  
```



##### （2）配置 systemd cgroup 驱动

https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/

> 警告：
>
> 你需要确保容器运行时和 kubelet 所使用的是相同的 cgroup 驱动，否则 kubelet 进程会失败

[容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes)页面提到， 由于 kubeadm 把 kubelet 视为一个 [系统服务](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration)来管理， 所以对基于 kubeadm 的安装， 我们推荐使用 `systemd` 驱动， 不推荐 kubelet [默认](https://kubernetes.io/zh-cn/docs/reference/config-api/kubelet-config.v1beta1)的 `cgroupfs` 驱动。



kubeadm 支持在执行 `kubeadm init` 时，传递一个 `KubeletConfiguration` 结构体。 `KubeletConfiguration` 包含 `cgroupDriver` 字段，可用于控制 kubelet 的 cgroup 驱动。

> 说明：
>
> 在版本 1.22 及更高版本中，如果用户没有在 `KubeletConfiguration` 中设置 `cgroupDriver` 字段， `kubeadm` 会将它设置为默认值 `systemd`。



#### 5. 使用 kubeadm 创建集群

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

使用 `kubeadm`，你能创建一个符合最佳实践的最小化 Kubernetes 集群。 

事实上，你可以使用 `kubeadm` 配置一个通过 [Kubernetes 一致性测试](https://kubernetes.io/blog/2017/10/software-conformance-certification/)的集群。

 `kubeadm` 还支持其他集群生命周期功能， 例如[启动引导令牌](https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/bootstrap-tokens/)和集群升级。



##### （1）准备所需的容器镜像

这个步骤是可选的，只适用于你希望 `kubeadm init` 和 `kubeadm join` 不去下载存放在 `registry.k8s.io` 上的默认容器镜像的情况。

当你在离线的节点上创建一个集群的时候，kubeadm 有一些命令可以帮助你预拉取所需的镜像。 阅读[离线运行 kubeadm](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init#without-internet-connection) 获取更多的详情。

kubeadm 允许你给所需要的镜像指定一个自定义的镜像仓库。 阅读[使用自定义镜像](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images)获取更多的详情。



##### （2）初始化控制平面节点

控制平面节点是运行控制平面组件的机器， 包括 [etcd](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/)（集群数据库） 和 [API 服务器](https://kubernetes.io/zh-cn/docs/concepts/architecture/#kube-apiserver) （命令行工具 [kubectl](https://kubernetes.io/zh-cn/docs/reference/kubectl/) 与之通信）。

1. （推荐）如果计划将单个控制平面 kubeadm 集群升级成[高可用](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/)， 你应该指定 `--control-plane-endpoint` 为所有控制平面节点设置共享端点。 端点可以是负载均衡器的 DNS 名称或 IP 地址。
2. 选择一个 Pod 网络插件，并验证是否需要为 `kubeadm init` 传递参数。 根据你选择的第三方网络插件，你可能需要设置 `--pod-network-cidr` 的值。 请参阅[安装 Pod 网络附加组件](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network)。
3. （可选）`kubeadm` 试图通过使用已知的端点列表来检测容器运行时。 使用不同的容器运行时或在预配置的节点上安装了多个容器运行时，请为 `kubeadm init` 指定 `--cri-socket` 参数。 请参阅[安装运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime)。

(

如果计划将单个控制平面 kubeadm 集群升级成高可用， 你应该指定 --control-plane-endpoint 为所有控制平面节点设置共享端点。 端点可以是负载均衡器的 DNS 名称或 IP 地址。

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/

使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：

- 使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。
- 使用外部 etcd 集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。

**堆叠（Stacked）etcd 拓扑**

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology

你应该为 HA 集群运行至少三个堆叠的控制平面节点。

这是 kubeadm 中的默认拓扑。当使用 `kubeadm init` 和 `kubeadm join --control-plane` 时， 在控制平面节点上会自动创建本地 etcd 成员。

![image-20250413154457798](2025-05-09-在 Ubuntu 24.04 中使用 kubeadm 安装 Kubernetes.assets/image-20250413154457798.png)

**外部 etcd 拓扑**

具有外部 etcd 的 HA 集群是一种这样的[拓扑](https://zh.wikipedia.org/wiki/网络拓扑)， 其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。

![image-20250413154831343](2025-05-09-在 Ubuntu 24.04 中使用 kubeadm 安装 Kubernetes.assets/image-20250413154831343.png)

)



```shell
# https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init/
# 打印默认初始化配置
kubeadm config print init-defaults > kubeadm-init.yaml

# 提前拉取镜像
# 创建代理配置目录
sudo mkdir -p /etc/systemd/system/containerd.service.d
# 添加代理配置文件
cat << EOF | sudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.31.72:7890"
Environment="HTTPS_PROXY=http://192.168.31.72:7890"
Environment="NO_PROXY=localhost,127.0.0.1/8,10.0.0.0/8,192.168.0.0/16"
EOF
# 重启 containerd 使配置生效
sudo systemctl daemon-reload
sudo systemctl restart containerd
# 显示镜像列表
kubeadm config images list --v=5
# 拉取镜像
sudo kubeadm config images pull --v=5


# 清理集群
#ubuntu@node1:~$ sudo kubeadm reset
# 初始化控制平面节点
# Pod 网络插件使用 flannel，flannel 中网络设置默认为 10.244.0.0/16
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# 配置 Kubernetes 的客户端工具 kubectl，以便用户管理 Kubernetes 集群
export KUBECONFIG=/etc/kubernetes/admin.conf
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


# 在安装网络之前，集群 DNS (CoreDNS) 将不会启动
ubuntu@node1:~$ kubectl get pods -A
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE
kube-system   coredns-668d6bf9bc-9tvb5        0/1     Pending   0          80m
kube-system   coredns-668d6bf9bc-lg9sp        0/1     Pending   0          80m
kube-system   etcd-node1                      1/1     Running   0          80m
kube-system   kube-apiserver-node1            1/1     Running   0          80m
kube-system   kube-controller-manager-node1   1/1     Running   0          80m
kube-system   kube-proxy-7lp7r                1/1     Running   0          80m
kube-system   kube-scheduler-node1            1/1     Running   0          80m

```

输出示例

```shell
ubuntu@node1:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.32.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
W0413 16:13:37.336214    4537 checks.go:846] detected that the sandbox image "" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.10" as the CRI sandbox image.


ubuntu@node1:~$ sudo kubeadm reset
W0413 16:35:51.668452    4624 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0413 16:35:54.301367    4624 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.


ubuntu@node1:~$ kubeadm config images list --v=5
I0413 16:38:33.060605    4635 initconfiguration.go:115] skip CRI socket detection, fill with the default CRI socket unix:///var/run/containerd/containerd.sock
I0413 16:38:33.060791    4635 interface.go:432] Looking for default routes with IPv4 addresses
I0413 16:38:33.060837    4635 interface.go:437] Default route transits interface "ens3"
I0413 16:38:33.060952    4635 interface.go:209] Interface ens3 is up
I0413 16:38:33.061019    4635 interface.go:257] Interface "ens3" has 2 addresses :[10.227.94.45/24 fe80::5054:ff:fec8:e37a/64].
I0413 16:38:33.061031    4635 interface.go:224] Checking addr  10.227.94.45/24.
I0413 16:38:33.061058    4635 interface.go:231] IP found 10.227.94.45
I0413 16:38:33.061095    4635 interface.go:263] Found valid IPv4 address 10.227.94.45 for interface "ens3".
I0413 16:38:33.061102    4635 interface.go:443] Found active IP 10.227.94.45 
I0413 16:38:33.061123    4635 kubelet.go:196] the value of KubeletConfiguration.cgroupDriver is empty; setting it to "systemd"
I0413 16:38:33.061151    4635 version.go:192] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt
registry.k8s.io/kube-apiserver:v1.32.3
registry.k8s.io/kube-controller-manager:v1.32.3
registry.k8s.io/kube-scheduler:v1.32.3
registry.k8s.io/kube-proxy:v1.32.3
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
registry.k8s.io/etcd:3.5.16-0

ubuntu@node1:~$ sudo kubeadm config images pull --v=5
I0413 18:13:20.735634    4995 initconfiguration.go:123] detected and using CRI socket: unix:///var/run/containerd/containerd.sock
I0413 18:13:20.735890    4995 interface.go:432] Looking for default routes with IPv4 addresses
I0413 18:13:20.735904    4995 interface.go:437] Default route transits interface "ens3"
I0413 18:13:20.735995    4995 interface.go:209] Interface ens3 is up
I0413 18:13:20.736059    4995 interface.go:257] Interface "ens3" has 2 addresses :[10.227.94.45/24 fe80::5054:ff:fec8:e37a/64].
I0413 18:13:20.736073    4995 interface.go:224] Checking addr  10.227.94.45/24.
I0413 18:13:20.736081    4995 interface.go:231] IP found 10.227.94.45
I0413 18:13:20.736092    4995 interface.go:263] Found valid IPv4 address 10.227.94.45 for interface "ens3".
I0413 18:13:20.736099    4995 interface.go:443] Found active IP 10.227.94.45 
I0413 18:13:20.736121    4995 kubelet.go:196] the value of KubeletConfiguration.cgroupDriver is empty; setting it to "systemd"
I0413 18:13:20.736187    4995 version.go:192] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt
[config/images] Pulled registry.k8s.io/kube-apiserver:v1.32.3
[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.32.3
[config/images] Pulled registry.k8s.io/kube-scheduler:v1.32.3
[config/images] Pulled registry.k8s.io/kube-proxy:v1.32.3
[config/images] Pulled registry.k8s.io/coredns/coredns:v1.11.3
[config/images] Pulled registry.k8s.io/pause:3.10
[config/images] Pulled registry.k8s.io/etcd:3.5.16-0
ubuntu@node1:~$ sudo systemctl edit containerd


ubuntu@node1:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.32.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
W0413 18:44:59.944938    5088 checks.go:846] detected that the sandbox image "" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.10" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local node1] and IPs [10.96.0.1 10.227.94.45]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost node1] and IPs [10.227.94.45 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost node1] and IPs [10.227.94.45 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.663151ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 4.50338602s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node node1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node node1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: duj9t8.a7kgj464k761bxs7
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.227.94.45:6443 --token duj9t8.a7kgj464k761bxs7 \
        --discovery-token-ca-cert-hash sha256:455dc33c126de427a68cf15fb512d248bdd5e364ffe46ecafb828a6a9b3d4eb9 
```



##### （3）安装 Pod 网络附加组件

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network

https://github.com/flannel-io/flannel/

你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件：

```shell
# 查看所有节点的 podCIDR
kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'

# 如果 podCIDR 不是 10.244.0.0/16，需要按 podCIDR 修改 kube-flannel.yml中的网络
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

每个集群只能安装一个 Pod 网络。

安装 Pod 网络后，你可以通过在 `kubectl get pods --all-namespaces` 输出中检查 CoreDNS Pod 是否 `Running` 来确认其是否正常运行。 一旦 CoreDNS Pod 启用并运行，你就可以继续加入节点。

如果你的网络无法正常工作或 CoreDNS 不在 `Running` 状态，请查看 `kubeadm` 的[故障排除指南](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)。



输出示例

```shell
ubuntu@node1:~$ kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
namespace/kube-flannel created
serviceaccount/flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```



##### （4）添加工作节点

https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/adding-linux-nodes/

###### ① 安装和配置先决条件

**启用 IPv4 数据包转发**

同上



###### ② 安装容器运行时 containerd

同上，安装容器运行时 containerd



###### ③ 安装 kubeadm、kubelet 和 kubectl

同上，安装 kubeadm、kubelet 和 kubectl



###### ④ 加入集群

如果你成功安装了 Master 节点，那么 Worker 节点的安装就简单多了，只需要用之前拷贝的那条 kubeadm join 命令就可以了，记得要用 sudo 来执行。

它会连接 Master 节点，然后拉取镜像，安装网络插件，最后把节点加入集群。

当然，这个过程中同样也会遇到拉取镜像的问题，你可以如法炮制，提前把镜像下载到 Worker 节点本地，这样安装过程中就不会再有障碍了。

Worker 节点安装完毕后，执行 kubectl get node ，就会看到两个节点都是“Ready”状态：

```shell
# 配置代理，避免拉取镜像因网络不通失败
# 创建代理配置目录
sudo mkdir -p /etc/systemd/system/containerd.service.d
# 添加代理配置文件
cat << EOF | sudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.31.72:7890"
Environment="HTTPS_PROXY=http://192.168.31.72:7890"
Environment="NO_PROXY=localhost,127.0.0.1/8,10.0.0.0/8,192.168.0.0/16"
EOF
# 重启 containerd 使配置生效
sudo systemctl daemon-reload
sudo systemctl restart containerd


# 将新的 Linux 工作节点添加到集群中
ubuntu@node2:~$ sudo kubeadm join 10.227.94.45:6443 --token duj9t8.a7kgj464k761bxs7 \
        --discovery-token-ca-cert-hash sha256:455dc33c126de427a68cf15fb512d248bdd5e364ffe46ecafb828a6a9b3d4eb9 	
        
ubuntu@node3:~$ sudo kubeadm join 10.227.94.45:6443 --token duj9t8.a7kgj464k761bxs7 \
        --discovery-token-ca-cert-hash sha256:455dc33c126de427a68cf15fb512d248bdd5e364ffe46ecafb828a6a9b3d4eb9        


# 在控制平面节点查看节点状态
ubuntu@node1:~$ kubectl get nodes
ubuntu@node1:~$ kubectl get pods -A -o wide

# 集群节点通常是按顺序初始化的，因此 CoreDNS Pods 可能会全部运行在第一个控制平面节点上。
# 重新平衡 CoreDNS Pods
ubuntu@node1:~$ kubectl -n kube-system rollout restart deployment coredns
```

输出示例

```shell
ubuntu@node2:~$ sudo kubeadm join 10.227.94.45:6443 --token duj9t8.a7kgj464k761bxs7 \
        --discovery-token-ca-cert-hash sha256:455dc33c126de427a68cf15fb512d248bdd5e364ffe46ecafb828a6a9b3d4eb9    
[preflight] Running pre-flight checks
[preflight] Reading configuration from the "kubeadm-config" ConfigMap in namespace "kube-system"...
[preflight] Use 'kubeadm init phase upload-config --config your-config.yaml' to re-upload it.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 500.502488ms
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


ubuntu@node1:~$ kubectl get nodes
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   16h   v1.32.3
node2   Ready    <none>          13h   v1.32.3
node3   Ready    <none>          53m   v1.32.3

ubuntu@node1:~$ kubectl get pods -A -o wide
NAMESPACE      NAME                            READY   STATUS    RESTARTS       AGE   IP              NODE    NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-ftv8q           1/1     Running   0              53m   10.227.94.236   node3   <none>           <none>
kube-flannel   kube-flannel-ds-gxzb2           1/1     Running   0              12h   10.227.94.229   node2   <none>           <none>
kube-flannel   kube-flannel-ds-sq45g           1/1     Running   22 (13h ago)   14h   10.227.94.45    node1   <none>           <none>
kube-system    coredns-cb9d6f8c-t2rwh          1/1     Running   0              12h   10.244.1.2      node2   <none>           <none>
kube-system    coredns-cb9d6f8c-xts89          1/1     Running   0              12h   10.244.1.3      node2   <none>           <none>
kube-system    etcd-node1                      1/1     Running   0              16h   10.227.94.45    node1   <none>           <none>
kube-system    kube-apiserver-node1            1/1     Running   0              16h   10.227.94.45    node1   <none>           <none>
kube-system    kube-controller-manager-node1   1/1     Running   0              16h   10.227.94.45    node1   <none>           <none>
kube-system    kube-proxy-7lp7r                1/1     Running   0              16h   10.227.94.45    node1   <none>           <none>
kube-system    kube-proxy-k9g9s                1/1     Running   0              13h   10.227.94.229   node2   <none>           <none>
kube-system    kube-proxy-qs2hv                1/1     Running   0              53m   10.227.94.236   node3   <none>           <none>
kube-system    kube-scheduler-node1            1/1     Running   0              16h   10.227.94.45    node1   <none>           <none>

ubuntu@node1:~$ kubectl -n kube-system rollout restart deployment coredns
deployment.apps/coredns restarted
```



##### （5）（可选）从控制平面节点以外的计算机控制集群

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#optional-controlling-your-cluster-from-machines-other-than-the-control-plane-node

为了使 kubectl 在其他计算机（例如笔记本电脑）上与你的集群通信， 你需要将管理员 kubeconfig 文件从控制平面节点复制到工作站，如下所示：

```bash
scp root@<control-plane-host>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
```

> admin.conf 文件为用户提供了对集群的超级用户特权。 该文件应谨慎使用。对于普通用户，建议生成一个你为其授予特权的唯一证书。 你可以使用 `kubeadm kubeconfig user --client-name <CN>` 命令执行此操作。 该命令会将 KubeConfig 文件打印到 STDOUT，你应该将其保存到文件并分发给用户。 之后，使用 `kubectl create (cluster)rolebinding` 授予特权。



##### （6）（可选）将 API 服务器代理到本地主机

如果你要从集群外部连接到 API 服务器，则可以使用 `kubectl proxy`：

```bash
scp root@<control-plane-host>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
```

你现在可以在 `http://localhost:8001/api/v1` 从本地访问 API 服务器。



##### （7）局限性

###### ① 集群弹性

此处创建的集群具有单个控制平面节点，运行单个 etcd 数据库。 这意味着如果控制平面节点发生故障，你的集群可能会丢失数据并且可能需要从头开始重新创建。

解决方法：

- 定期[备份 etcd](https://etcd.io/docs/v3.5/op-guide/recovery/)。 kubeadm 配置的 etcd 数据目录位于控制平面节点上的 `/var/lib/etcd` 中。

- 使用多个控制平面节点。 你可以阅读[可选的高可用性拓扑](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/)选择集群拓扑提供的 [高可用性](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/)。





### 04 接下来

#### 1. 使用 Sonobuoy 验证集群是否正常运行

https://github.com/vmware-tanzu/sonobuoy

```shell
# 安装
ubuntu@node1:~$ curl -L -O https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.57.3/sonobuoy_0.57.3_linux_amd64.tar.gz
ubuntu@node1:~$ tar -zxvf sonobuoy_0.57.3_linux_amd64.tar.gz 

# 运行
ubuntu@node1:~$ ./sonobuoy run --wait
#ubuntu@node1:~$ ./sonobuoy --mode quick

# 获取结果
ubuntu@node1:~$ results=$(./sonobuoy retrieve)
ubuntu@node1:~$ sonobuoy results $results
Run Details:
API Server version: v1.32.3
Node health: 3/3 (100%)
Pods health: 17/17 (100%)

# 清理
ubuntu@node1:~$ ./sonobuoy delete --wait
Namespace "sonobuoy" has been deleted
```



#### 2. 安装可视化管理扩展 Dashboard

https://github.com/kubernetes/dashboard#kubernetes-dashboard

Dashboard 是一个 Kubernetes 的 Web 控制台界面。



##### （1）安装 Helm

https://helm.sh/docs/intro/install/

每个Helm [版本](https://github.com/helm/helm/releases)都提供了各种操作系统的二进制版本，这些版本可以手动下载和安装。

1. 下载 [需要的版本](https://github.com/helm/helm/releases)
2. 解压(`tar -zxvf helm-v3.0.0-linux-amd64.tar.gz`)
3. 在解压目录中找到`helm`程序，移动到需要的目录中(`mv linux-amd64/helm /usr/local/bin/helm`)

```shell
ubuntu@node1:~$ wget https://get.helm.sh/helm-v3.17.3-linux-amd64.tar.gz

ubuntu@node1:~$ tar -zxvf helm-v3.17.2-linux-amd64.tar.gz 
linux-amd64/
linux-amd64/LICENSE
linux-amd64/helm
linux-amd64/README.md

ubuntu@node1:~$ sudo mv linux-amd64/helm /usr/local/bin/helm
ubuntu@node1:~$ helm help
The Kubernetes package manager

Common actions for Helm:

- helm search:    search for charts
- helm pull:      download a chart to your local directory to view
- helm install:   upload the chart to Kubernetes
- helm list:      list releases of charts
```



##### （2）安装 Kubernetes Dashboard

https://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard/

https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard

https://github.com/kubernetes/dashboard

> 说明：
>
> Kubernetes Dashboard 目前仅支持基于 Helm 的安装，因为它速度更快， 并且可以让我们更好地控制 Dashboard 运行所需的所有依赖项。

```shell
# 当前 shell
export HTTP_PROXY=http://192.168.31.72:7890/
export HTTPS_PROXY=http://192.168.31.72:7890/

# 添加 kubernetes-dashboard 仓库
ubuntu@node1:~$ helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
# 使用 kubernetes-dashboard Chart 部署名为 `kubernetes-dashboard` 的 Helm Release
#ubuntu@node1:~$ unset no_proxy https_proxy NO_PROXY HTTPS_PROXY HTTP_PROXY http_proxy ALL_PROXY all_proxy
# --install: if a release by this name doesn't already exist, run an install
ubuntu@node1:~$ helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard --debug


ubuntu@node1:~$ helm list -A
NAME                    NAMESPACE               REVISION        UPDATED                                        STATUS          CHART                           APP VERSION
kubernetes-dashboard    kubernetes-dashboard    2               2025-03-24 16:50:50.782651819 +0800 CST        deployed        kubernetes-dashboard-7.11.1

ubuntu@node1:~$ kubectl -n kubernetes-dashboard get pods
ubuntu@node1:~$ kubectl -n kubernetes-dashboard describe pod kubernetes-dashboard-api-6ddf9cfb58-rz5km 
# 删除 dashboard
#kubectl delete ns kubernetes-dashboard
# 强制删除旧的 Pod，触发重新拉取镜像：
# kubectl -n kubernetes-dashboard delete pod --all


# To access Dashboard run:
#kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
# 本机访问
# unset HTTP_PROXY http_proxy HTTPS_PROXY https_proxy ALL_PROXY all_proxy
kubectl -n kubernetes-dashboard port-forward --address 10.227.94.45 svc/kubernetes-dashboard-kong-proxy 8443:443

# Web UI
https://10.227.94.45:8443/


# 创建用户
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md
ubuntu@node1:~$ vim dashboard-adminuser.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: molihua
  namespace: kubernetes-dashboard
  
ubuntu@node1:~$ vim dashboard-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: molihua
  namespace: kubernetes-dashboard
  
  
ubuntu@node1:~$ kubectl apply -f dashboard-adminuser.yaml 
serviceaccount/molihua created
ubuntu@node1:~$ kubectl apply -f dashboard-role.yaml 
clusterrolebinding.rbac.authorization.k8s.io/admin-user created


ubuntu@node1:~$ kubectl -n kubernetes-dashboard create token molihua
eyJhbGciOiJSUzI1NiIsImtpZCI6Im03YjZIdDRwaEx2MVRXQWdhYmFvcG92a1lwZmNTZ3VLRFJnTXY0Y3ZDbjgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQ1MDc2MDY1LCJpYXQiOjE3NDUwNzI0NjUsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiMjFkZDAyN2UtZDJkNC00N2ZmLThhOWYtMjVlZDg2YzMwOWFlIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJtb2xpaHVhIiwidWlkIjoiYTA5NGZkOWYtNGFiZC00Yjk1LWI2MzAtMDk2MTgzOGE5OWI1In19LCJuYmYiOjE3NDUwNzI0NjUsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDptb2xpaHVhIn0.SF5IW9SnYeZ_t3jOrUBsGAi0FzCs1y7RkSQFnMkVMPVg1lEotqr9cOq3t9pmuiJ6jnZNESwsh7K8sqhTglkrUPBHzfSSDyQDP4EzbJf4NFtxNuWu7QEfbBoNX62T_8zbW4bORPlyEN6AwMFpALRaTktrKbD8hYU_WnP_CuGWq9nEBE2fb5bed29svVOx7fMyLbHVNb1uUZhlKTdC7u4RRCzj5R6Fo-VtJz8MdFkg6J6ZH7yFEp7MNjnnWhFAq-d_dsPZm1dMh-EFp21joCJkTeVZvzfWDwxqC6ok8niyPMQz-8e6EW1dE3nb8-C-bT_LV1JdObO51oCIKb0BHlKMJg
```

输出示例

```shell
ubuntu@node1:~$ helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard --debug
  
NOTES:
*************************************************************************************************
*** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***
*************************************************************************************************

Congratulations! You have just installed Kubernetes Dashboard in your cluster.

To access Dashboard run:
  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443

NOTE: In case port-forward command does not work, make sure that kong service name is correct.
      Check the services in Kubernetes Dashboard namespace using:
        kubectl -n kubernetes-dashboard get svc

Dashboard will be available at:
  https://localhost:8443  

```



#### 3. 通过 Nginx  访问 Kubernetes Dashboard

使用 Deployment 运行一个无状态应用

https://kubernetes.io/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/

https://time.geekbang.org/column/article/536829



##### （1）配置文件

###### nginx-configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: default
data:
  nginx.conf: |
    events {
      worker_connections 1024;
    }

    http {
      log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

      access_log /var/log/nginx/access.log main;

      server {
        listen 80;
        # 重定向 HTTP 到 HTTPS（如果您想要同时支持两者）
        return 301 https://$host$request_uri;    
      }


      server {
        listen 443 ssl;  # Nginx 监听 443 端口，启用 SSL
        server_name dashboard.example.com;  # 设置你的域名或 IP
        
        ssl_certificate /etc/nginx/ssl/tls.crt;
        ssl_certificate_key /etc/nginx/ssl/tls.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        
        location / {
          proxy_pass https://kubernetes-dashboard-kong-proxy.kubernetes-dashboard.svc.cluster.local:443;
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header Authorization $http_authorization;  # 明确传递 Authorization 头
          proxy_ssl_verify off;

          # WebSocket 支持（Dashboard 需要）
          proxy_http_version 1.1;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";          
        }
      }
    }
```

###### nginx-deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.26.3
        ports:
        - containerPort: 443
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        - name: ssl-certs
          mountPath: /etc/nginx/ssl    
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
      - name: ssl-certs
        secret:
          secretName: nginx-tls-secret
```

###### nginx-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-proxy
  namespace: default
spec:
  selector:
    app: nginx
  ports:
  - port: 443
    targetPort: 443
    protocol: TCP
  type: NodePort  # 或者使用 LoadBalancer 或 ClusterIP，根据需求
```

```yaml
如果您希望同时支持 HTTP 和 HTTPS，可以在 Nginx 配置中添加 HTTP 端口监听并重定向到 HTTPS，然后在 Service 中添加两个端口映射：
spec:
  selector:
    app: nginx
  ports:
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP      
  type: NodePort  # 或者使用 LoadBalancer 或 ClusterIP，根据需求      
```



##### （2）运行

```shell
ubuntu@node1:~$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout nginx.key -out nginx.crt \
  -subj "/CN=dashboard.local"
ubuntu@node1:~$ kubectl create secret tls nginx-tls-secret \
  --key nginx.key \
  --cert nginx.crt \
  --namespace default
secret/nginx-tls-secret created

ubuntu@node1:~$ kubectl apply -f nginx-configmap.yaml
ubuntu@node1:~$ kubectl apply -f nginx-deployment.yaml
ubuntu@node1:~$ kubectl apply -f nginx-service.yaml
```



###### 访问 Kubernetes Dashboard

```shell
ubuntu@node1:~$ kubectl get nodes -o wide
NAME    STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
node1   Ready    control-plane   6d3h    v1.32.3   10.227.94.45    <none>        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://2.0.4
node2   Ready    <none>          6d1h    v1.32.3   10.227.94.229   <none>        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://2.0.4
node3   Ready    <none>          5d12h   v1.32.3   10.227.94.236   <none>        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://2.0.4

ubuntu@node1:~$ kubectl get svc nginx-dashboard-proxy -n default
NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
nginx-dashboard-proxy   NodePort   10.97.110.24   <none>        443:30745/TCP   137m

# 通过 https://<节点IP>:<NodePort> 访问 Kubernetes Dashboard
https://10.227.94.45:30745/
```



输出示例

```shell
ubuntu@node1:~$ kubectl apply -f nginx-configmap.yaml 
ubuntu@node1:~$ kubectl get pods
NAME                                READY   STATUS             RESTARTS     AGE
nginx-deployment-54b7f955cd-v9f5w   0/1     CrashLoopBackOff   6 (3m ago)   8m54s
nginx-deployment-674b4585dd-gp5qh   1/1     Running            0            3h51m
nginx-deployment-674b4585dd-hv27v   1/1     Running            0            3h51m

ubuntu@node1:~$ kubectl logs nginx-deployment-54b7f955cd-v9f5w
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up

# 修改 nginx-configmap.yaml 增加 events 块
ubuntu@node1:~$ vim nginx-configmap.yaml 
events {
    worker_connections 1024;  # 可以根据需要调整
}
ubuntu@node1:~$ kubectl apply -f nginx-configmap.yaml 

# 修改 nginx-configmap.yaml 后， pod 正常创建
ubuntu@node1:~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS      AGE
nginx-deployment-54b7f955cd-m6mhg   1/1     Running   0             5m43s
nginx-deployment-54b7f955cd-v9f5w   1/1     Running   7 (10m ago)   16m
# 查看 nginx-deployment
ubuntu@node1:~$ kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sat, 19 Apr 2025 15:51:39 +0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=nginx
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.26.3
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /etc/nginx/nginx.conf from nginx-config (rw,path="nginx.conf")
  Volumes:
   nginx-config:
    Type:          ConfigMap (a volume populated by a ConfigMap)
    Name:          nginx-config
    Optional:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  nginx-deployment-674b4585dd (0/0 replicas created)
NewReplicaSet:   nginx-deployment-54b7f955cd (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  12m   deployment-controller  Scaled up replica set nginx-deployment-54b7f955cd from 0 to 1
  Normal  ScalingReplicaSet  79s   deployment-controller  Scaled down replica set nginx-deployment-674b4585dd from 2 to 1
  Normal  ScalingReplicaSet  79s   deployment-controller  Scaled up replica set nginx-deployment-54b7f955cd from 1 to 2
  Normal  ScalingReplicaSet  78s   deployment-controller  Scaled down replica set nginx-deployment-674b4585dd from 1 to 0
```





#### 4. 停止运行 K8s

1.使用 **kubeadm** 创建的集群，停止集群（所有节点）

```shell
sudo systemctl stop kubelet
sudo systemctl stop containerd
```

2.使用 **Multipass + kubeadm** 搭建的集群

可以直接停止每个虚拟机：

```shell
multipass stop node1 node2 node3
```



### 05 问题

#### 1. 拉取镜像失败

```shell
使用 kubeadm 安装Kubernetes， 容器运行时使用的是containerd，在拉取镜像出现如下问题，请问如何解决？ 

ubuntu@node1:~$ sudo kubeadm config images pull --v=5
I0413 16:40:27.341714    4653 initconfiguration.go:123] detected and using CRI socket: unix:///var/run/containerd/containerd.sock
I0413 16:40:27.342078    4653 interface.go:432] Looking for default routes with IPv4 addresses
I0413 16:40:27.342105    4653 interface.go:437] Default route transits interface "ens3"
I0413 16:40:27.342223    4653 interface.go:209] Interface ens3 is up
I0413 16:40:27.342494    4653 interface.go:257] Interface "ens3" has 2 addresses :[10.227.94.45/24 fe80::5054:ff:fec8:e37a/64].
I0413 16:40:27.342532    4653 interface.go:224] Checking addr  10.227.94.45/24.
I0413 16:40:27.342542    4653 interface.go:231] IP found 10.227.94.45
I0413 16:40:27.342557    4653 interface.go:263] Found valid IPv4 address 10.227.94.45 for interface "ens3".
I0413 16:40:27.342565    4653 interface.go:443] Found active IP 10.227.94.45 
I0413 16:40:27.342582    4653 kubelet.go:196] the value of KubeletConfiguration.cgroupDriver is empty; setting it to "systemd"
I0413 16:40:27.342664    4653 version.go:192] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt
rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "registry.k8s.io/kube-apiserver:v1.32.3": failed to resolve reference "registry.k8s.io/kube-apiserver:v1.32.3": failed to do request: Head "https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.32.3": dial tcp 142.250.157.82:443: i/o timeout
failed to pull image registry.k8s.io/kube-apiserver:v1.32.3
k8s.io/kubernetes/cmd/kubeadm/app/util/runtime.(*CRIRuntime).PullImage
        k8s.io/kubernetes/cmd/kubeadm/app/util/runtime/runtime.go:178
k8s.io/kubernetes/cmd/kubeadm/app/cmd.PullControlPlaneImages
        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:404
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1
        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:390
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
k8s.io/kubernetes/cmd/kubeadm/app.Run
        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47
main.main
        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
failed to pull image "registry.k8s.io/kube-apiserver:v1.32.3"
k8s.io/kubernetes/cmd/kubeadm/app/cmd.PullControlPlaneImages
        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:405
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdConfigImagesPull.func1
        k8s.io/kubernetes/cmd/kubeadm/app/cmd/config.go:390
github.com/spf13/cobra.(*Command).execute
        github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
        github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
        github.com/spf13/cobra@v1.8.1/command.go:1041
k8s.io/kubernetes/cmd/kubeadm/app.Run
        k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:47
main.main
        k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
        runtime/proc.go:272
runtime.goexit
        runtime/asm_amd64.s:1700
        
        
ubuntu@node1:~$ sudo ctr image pull registry.k8s.io/kube-apiserver:v1.32.3
ctr: rpc error: code = DeadlineExceeded desc = failed to resolve image: failed to do request: Head "https://asia-east1-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.32.3": dial tcp 142.250.157.82:443: i/o timeout        
```

解决方案

export 设置的环境变量只对当前 shell 会话及其直接子进程有效，但拉取镜像时并没有走代理，而 containerd 通常作为系统服务由 systemd 管理，systemd 在启动服务时，会为服务进程创建一个独立的环境，仅使用服务配置文件中定义的环境变量，默认不会继承当前 shell 的环境变量。

通过配置文件为 containerd 设置环境变量。

```shell
# 创建代理配置目录
sudo mkdir -p /etc/systemd/system/containerd.service.d

# 添加代理配置文件
cat << EOF | sudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.31.72:7890"
Environment="HTTPS_PROXY=http://192.168.31.72:7890"
Environment="NO_PROXY=localhost,127.0.0.1/8,10.0.0.0/8,192.168.0.0/16"
EOF

# 重启 containerd 使配置生效
sudo systemctl daemon-reload
sudo systemctl restart containerd

```



#### 2. kube-flannel pod出现异常

```shell
ubuntu@node1:~$ kubectl get pods -A
NAMESPACE      NAME                            READY   STATUS              RESTARTS         AGE
kube-flannel   kube-flannel-ds-sq45g           0/1     CrashLoopBackOff    16 (4m25s ago)   62m
kube-flannel   kube-flannel-ds-wwsj8           0/1     CrashLoopBackOff    4 (36s ago)      2m43s
kube-system    coredns-668d6bf9bc-9tvb5        0/1     ContainerCreating   0                160m
kube-system    coredns-668d6bf9bc-lg9sp        0/1     ContainerCreating   0                160m
kube-system    etcd-node1                      1/1     Running             0                161m
kube-system    kube-apiserver-node1            1/1     Running             0                161m
kube-system    kube-controller-manager-node1   1/1     Running             0                161m
kube-system    kube-proxy-7lp7r                1/1     Running             0                160m
kube-system    kube-proxy-k9g9s                1/1     Running             0                2m43s
kube-system    kube-scheduler-node1            1/1     Running             0                161m

ubuntu@node1:~$ kubectl logs -n kube-flannel kube-flannel-ds-sq45g
Defaulted container "kube-flannel" out of: kube-flannel, install-cni-plugin (init), install-cni (init)
I0413 13:31:51.548722       1 main.go:211] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true}
W0413 13:31:51.549070       1 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0413 13:31:51.567241       1 kube.go:139] Waiting 10m0s for node controller to sync
I0413 13:31:51.568557       1 kube.go:469] Starting kube subnet manager
I0413 13:31:52.568967       1 kube.go:146] Node controller sync successful
I0413 13:31:52.569006       1 main.go:231] Created subnet manager: Kubernetes Subnet Manager - node1
I0413 13:31:52.569015       1 main.go:234] Installing signal handlers
I0413 13:31:52.569401       1 main.go:468] Found network config - Backend type: vxlan
E0413 13:31:52.569557       1 main.go:268] Failed to check br_netfilter: stat /proc/sys/net/bridge/bridge-nf-call-iptables: no such file or directory
```

解决方案

```shell
# 手动加载内核模块
sudo modprobe br_netfilter

# 确认模块已经加载
lsmod | grep br_netfilter

# 写入配置文件，系统重启后依然生效
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee -a /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 确认参数的设置
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward


# 查看 pod 情况，如果 pod 还是异常，删除pod触发自动重建
ubuntu@node1:~$ kubectl get pods -A -o wide
kubectl get pods -A
kubectl delete pod -n kube-flannel kube-flannel-ds-sq45g
kubectl delete pod -n kube-flannel kube-flannel-ds-wwsj8
```



#### 3. 添加工作节点

```shell
ubuntu@node3:~$ sudo kubeadm join 10.227.94.45:6443 --token duj9t8.a7kgj464k761bxs7 \
        --discovery-token-ca-cert-hash sha256:455dc33c126de427a68cf15fb512d248bdd5e364ffe46ecafb828a6a9b3d4eb9  

ubuntu@node1:~$ kubectl get nodes
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   15h   v1.32.3
node2   Ready    <none>          13h   v1.32.3
node3   NotReady    <none>       35m   v1.32.3


ubuntu@node1:~$ kubectl get pods -A -o wide
NAMESPACE      NAME                            READY   STATUS              RESTARTS       AGE     IP              NODE    NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-ftv8q           0/1     Init:0/2            0              2m47s   10.227.94.236   node3   <none>           <none>
kube-flannel   kube-flannel-ds-gxzb2           1/1     Running             0              11h     10.227.94.229   node2   <none>           <none>
kube-flannel   kube-flannel-ds-sq45g           1/1     Running             22 (12h ago)   13h     10.227.94.45    node1   <none>           <none>
kube-system    coredns-cb9d6f8c-t2rwh          1/1     Running             0              11h     10.244.1.2      node2   <none>           <none>
kube-system    coredns-cb9d6f8c-xts89          1/1     Running             0              11h     10.244.1.3      node2   <none>           <none>
kube-system    etcd-node1                      1/1     Running             0              15h     10.227.94.45    node1   <none>           <none>
kube-system    kube-apiserver-node1            1/1     Running             0              15h     10.227.94.45    node1   <none>           <none>
kube-system    kube-controller-manager-node1   1/1     Running             0              15h     10.227.94.45    node1   <none>           <none>
kube-system    kube-proxy-7lp7r                1/1     Running             0              15h     10.227.94.45    node1   <none>           <none>
kube-system    kube-proxy-k9g9s                1/1     Running             0              12h     10.227.94.229   node2   <none>           <none>
kube-system    kube-proxy-qs2hv                0/1     ContainerCreating   0              2m47s   10.227.94.236   node3   <none>           <none>
kube-system    kube-scheduler-node1            1/1     Running             0              15h     10.227.94.45    node1   <none>           <none>

# 查看 Kubernetes 节点 node3 的详细信息
ubuntu@node1:~$ kubectl describe node node3
# 查看 Pod 的详细信息
ubuntu@node1:~$ kubectl describe pod kube-flannel-ds-ftv8q -n kube-flannel
ubuntu@node1:~$ kubectl describe pod kube-proxy-qs2hv -n kube-system
# 查看容器的日志
ubuntu@node1:~$ kubectl logs kube-flannel-ds-ftv8q -n kube-flannel

# 查看服务的日志
ubuntu@node3:~$ sudo journalctl -u kubelet
ubuntu@node3:~$ sudo journalctl -u containerd

# 查看服务的状态
ubuntu@node3:~$ sudo systemctl status kubelet
Apr 14 10:10:16 node3 kubelet[2794]: E0414 10:10:16.267727    2794 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to start sandbox \"65cc1762dd8312db2979522a2e0827e3f8a3e9ad4a8de74d3dbac18086fd8456\": failed to get sandbox image \"registry.k8s.io/pause:3.10\": failed to pull image \"registry.k8s.io/pause:3.10\": failed to pull and unpack image \"registry.k8s.io/pause:3.10\": failed to resolve reference \"registry.k8s.io/pause:3.10\": failed to do request: Head \"https://europe-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.10\": dial tcp 142.250.157.82:443: i/o timeout"
ubuntu@node3:~$ sudo systemctl status containerd
Apr 14 10:10:58 node3 containerd[1492]: time="2025-04-14T10:10:58.154344541+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-flannel-ds-ftv8q,Uid:bf70e189-a1c6-43ad-9588-349b2c10554d,Namespace:kube-flannel,Attempt:0,} failed, error" error="rpc error: code = DeadlineExceeded desc = failed to start sandbox \"638c4b2460ead245404ca7fb151279fa2dcdd960a7c8aa92353910e9a93de9d1\": failed to get sandbox image \"registry.k8s.io/pause:3.10\": failed to pull image \"registry.k8s.io/pause:3.10\": failed to pull and unpack image \"registry.k8s.io/pause:3.10\": failed to resolve reference \"registry.k8s.io/pause:3.10\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.10\": dial tcp 142.250.157.82:443: i/o timeout"
```

解决方案

配置代理

```shell
ubuntu@node3:
# 配置代理，避免拉取镜像因网络不通失败
# 创建代理配置目录
sudo mkdir -p /etc/systemd/system/containerd.service.d
# 添加代理配置文件
cat << EOF | sudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.31.72:7890"
Environment="HTTPS_PROXY=http://192.168.31.72:7890"
Environment="NO_PROXY=localhost,127.0.0.1/8,10.0.0.0/8,192.168.0.0/16"
EOF
# 重启 containerd 使配置生效
sudo systemctl daemon-reload
sudo systemctl restart containerd

ubuntu@node1:~$ kubectl get nodes
ubuntu@node1:~$ kubectl get pods -A -o wide
```





### 06 参考资料

#### 1. 更真实的云原生：实际搭建多节点的Kubernetes集群

https://time.geekbang.org/column/article/534762





### 07 附录

#### **1. Kubernetes 学习资料**

文档

https://kubernetes.io/zh-cn/docs/concepts/overview/

课程

《深入剖析 Kubernetes》

https://time.geekbang.org/column/intro/100015201?tab=intro

《Kubernetes 入门实战课》

https://time.geekbang.org/column/intro/100114501?tab=intro

《Kubernetes 源码剖析与实战》

https://time.geekbang.com/column/intro/101031301
